import os
import numpy as np
import pandas as pd
import collections
import pickle
import sys
import psutil as ps
import random
import csv
import json
from utils.timeseries import TimeSeries

class Shapelet(object):
    def __init__(self):
        self.id = id(self)
        self.name = 0.0
        self.Class = ''
        self.subseq = None
        self.differ_distance = 0.0
        self.normal_distance = 0.0
        self.dist_threshold = 0.0
        # [ts_target_name1, ts_target_name2, ...], Array[string]
        self.matching_ts = []
        # {ts_target_name:[idx1,idx2,...]}, dict{String:Array[]}
        self.matching_indices = {}

class Dataset(object):
    def __init__(self):
        self.name = "test"
        self.ClassList = []
        self.size = 0
        self.tslength = 0
        self.queryLength = []
        self.queryIndex = []
        self.tsNameDir = {} #TS+number : Hash name of TS
        self.tsNbrList = [] #TS_number list

    def update(self, array_tsdict, datasetname):
        self.name = datasetname
        # "array_tsdict": [ {HashName: tsObject} ]
        self.tsObjectDir = {key: value for element in array_tsdict for key, value in element.items()}
        HushList = self.tsObjectDir.keys()
        for Hush in HushList:
            self.tsNameDir["ts" + str(self.size)] = Hush  # ts.name here is a hash number, should be converted into a simple interger
            self.ClassList.append(self.tsObjectDir[Hush].class_timeseries)
            self.size += 1
        self.tsNbrList = list(self.tsNameDir.keys())
        self.ClassList = list(set(self.ClassList))
        self.tslength = len(self.tsObjectDir[list(HushList)[0]].timeseries)
        self.queryLength = list(range(self.tslength//2))
        # the index will be changed along with the query's length
        #self.queryIndex = list(range(0, self.tslength - v_queryLength))

    def ts_object(self, ts_name):
        #return the TS object
        hush = self.tsNameDir(ts_name)
        return self.tsObjectDir(hush)

def load(directory, option):
    dirname = ""
    extension =""
    if option.lower() == "dataset":
        dirname = "/Dataset/"
        extension = ".csv"

    files_list = [f for f in os.listdir(directory + dirname) if f.lower().endswith(extension)]
    list_objects = []
    for file in files_list:
        path = directory + dirname + file
        #an_object = np.genfromtxt(path)
        an_object = np.genfromtxt(path, delimiter = ",")
        list_objects.extend(an_object)
    # return a list[Array_TimeSeries]
    return list_objects

# if the dataset format is changed, just modify this function to adapt it
def generate_timeseries(directory):
    #list_ts = list[TimeSeries]
    array_ts = []
    list_rawData = load(directory, 'Dataset')
    for d in list_rawData:
        # d[0] is the class of TS in the original data, d[1:] is the data in TS
        t = TimeSeries()
        t.class_timeseries = d[0]
        t.timeseries = d[1:]
        t.name = hash(d[1:].tostring())
        array_ts.append({t.name:t})
    return array_ts

# For loading UCR/UEA datasets, return a dictionary
def load_dataset(directory):
    #return: list of dictionary, each dict. has one single element {t.name:t.sequence}
    array_ts = []
    list_rawData = np.genfromtxt(directory, delimiter=",", dtype=object)
    np.random.shuffle(list_rawData)
    i = 1
    for d in list_rawData:
        # d[0] is the class of TS in the original data, d[1:] is the data in TS
        t = TimeSeries()
        t.class_timeseries = d[0]
        ### MODIFIED PART ###
        #t.timeseries = d[1:]
        #print(str(d[1])[2:-1].split(' '))
        #ts = [float(i) for i in str(d[1])[2:-1].split(' ')]
        ts = [float(str(d[idx])[2:-1]) for idx in range(len(d))]

        t.timeseries = np.asarray(ts)
        t.id = i
        t.name = hash(d[1:].tostring())
        array_ts.append({t.name:t})
        i += 1
    return array_ts

# For loading UCR/UEA datasets, return a list
def load_dataset_list(directory):
    #return: list of dictionary, each dict. has one single element {t.name:t.sequence}
    array_ts = []
    list_rawData = np.genfromtxt(directory, delimiter=",")
    np.random.shuffle(list_rawData)
    i = 1
    for d in list_rawData:
        # d[0] is the class of TS in the original data, d[1:] is the data in TS
        t = TimeSeries()
        t.class_timeseries = d[0]
        t.timeseries = d[1:]
        t.id = i
        t.name = hash(d[1:].tostring())
        array_ts.append(t)
        i += 1
    return array_ts

def print_progress(iteration, total, prefix='Progress:', suffix='Complete', decimals=1, barLength=70):
    """
    Call in a loop to create terminal progress bar
    @params:
        iteration   - Required  : current iteration (Int)
        total       - Required  : total iterations (Int)
        prefix      - Optional  : prefix string (Str)
        suffix      - Optional  : suffix string (Str)
        decimals    - Optional  : positive number of decimals in percent complete (Int)
        barLength   - Optional  : character length of bar (Int)
    """
    format_str = "{0:." + str(decimals) + "f}"
    percent = format_str.format(100 * (iteration / float(total)))
    filled_length = int(round(barLength * iteration / float(total)))
    bar = 'â–ˆ' * filled_length + '-' * (barLength - filled_length)
    sys.stdout.write('\r%s |%s| %s%s %s' % (prefix, bar, percent, '%', suffix)),
    if iteration == total:
        sys.stdout.write('\n')
    sys.stdout.flush()

def check_memory(perc=90):
    mem = ps.virtual_memory()
    if mem.percent >= perc:
        return False
    return True

def min_length_dataset(list_timeseries):
    min_l = sys.maxsize
    for mts in list_timeseries:
        if len(mts.timeseries) < min_l:
            min_l = len(mts.timeseries)
    return min_l

def save_shapelet(directory, shapList):
    output_shapelet = pd.DataFrame([[0, 0, 0, 0, 0]],
                               columns=['shap.name', 'shap.Class', 'shap.subseq', 'shap.score', 'shap.thresh'])

    ShapFolder = directory + 'SMAPShapelet'
    extension = ".csv"
    file_name = "/shapelet" + extension
    for shap in shapList:
        shap_set = [shap.name, shap.Class, str(shap.subseq), shap.normal_distance, shap.dist_threshold]
        shap_pd = pd.DataFrame([shap_set], columns=['shap.name', 'shap.Class', 'shap.subseq', 'shap.score', 'shap.thresh'])
        output_shapelet = output_shapelet.append(shap_pd, ignore_index = True)

    if not os.path.exists(ShapFolder):
        os.makedirs(ShapFolder)
    else:
        ##clean the historical files
        files_list = [f for f in os.listdir(ShapFolder) if f.lower().endswith(extension)]
        for file in files_list:
            path = ShapFolder + '/' + file
            os.remove(path)
    output_shapelet.to_csv(ShapFolder + file_name, index=False)
